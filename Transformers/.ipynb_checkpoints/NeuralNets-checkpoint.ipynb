{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ccee685-6d75-4678-bf0d-5872e6dcc513",
   "metadata": {},
   "source": [
    "### Transformers Architecture\n",
    "\n",
    "Transformers is the standard architecture upon which most LLMs are based—the decoder-only transformer architecture. This architecture is a modified version of the encoder-decoder transformer architecture that was popularized by GPT. \n",
    "\n",
    "While explaining the architecture, we will rely on Andrej Karpathy’s nanoGPT—a minimal and functional implementation of decoder-only transformers—as a reference.\n",
    "\n",
    "[1] https://github.com/karpathy/nanoGPT \n",
    "\n",
    "[2] https://github.com/wolfecameron/nanoMoE , https://cameronrwolfe.substack.com/p/nano-moe\n",
    "\n",
    "[3] https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "[4] https://poloclub.github.io/transformer-explainer/\n",
    "\n",
    "#<img src=\"e0c20b8a-b589-4282-8be7-e2509f4e0803_912x1112.png\" alt=\"transformers\" width=\"700\"/>\n",
    "\n",
    "![transformers](e0c20b8a-b589-4282-8be7-e2509f4e0803_912x1112.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d22cbb-317b-4e60-bf06-ea870ff8e5a0",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. The outputs of the self-attention layer are fed to a feed-forward neural network. \n",
    "\n",
    "We begin by turning each input word in the sequence into a vector using an embedding algorithm.\n",
    "\n",
    "<img src=\"embeddings.png\" alt=\"embeddings\">\n",
    "\n",
    "The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.\n",
    "\n",
    "After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.\n",
    "\n",
    "<img src=\"encoder_with_tensors.png\" alt=\"encode_tensors\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd1e24-36d2-47bf-afb9-47d8051f5354",
   "metadata": {},
   "source": [
    "#### Self-Attention\n",
    "\n",
    "Say the following sentence is an input sentence we want to translate:\n",
    "\n",
    "”The animal didn't cross the street because it was too tired”\n",
    "\n",
    "What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\n",
    "\n",
    "When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\n",
    "\n",
    "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
    "\n",
    "<img src=\"transformer_self-attention_visualization.png\" alt=\"self-attn\">\n",
    "\n",
    "##### Self-Attention in Detail\n",
    "\n",
    "The **first step** in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "\n",
    "Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\n",
    "\n",
    "<img src=\"transformer_self_attention_vectors.png\" alt=\"tattn\">\n",
    "\n",
    "Multiplying `x1` by the `WQ` weight matrix produces `q1`, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bd20-8fa2-418d-9f5f-c4e8bf9d2824",
   "metadata": {},
   "source": [
    "The **second step** in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "\n",
    "The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of `q1` and `k1`. The second score would be the dot product of `q1` and `k2`.\n",
    "\n",
    "<img src=\"transformer_self_attention_score.png\" alt=\"score1\">\n",
    "\n",
    "The **third and fourth steps** are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\n",
    "\n",
    "<img src=\"self-attention_softmax.png\" alt=\"score2\">\n",
    "\n",
    "This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fa35e-fc60-4ff8-8d91-61dc1c16977d",
   "metadata": {},
   "source": [
    "The **fifth step** is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
    "\n",
    "The **sixth step** is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
    "\n",
    "<img src=\"self-attention-output.png\" alt=\"attn-output\">\n",
    "\n",
    "That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af784ec5-b20c-4c78-9366-122d6484e688",
   "metadata": {},
   "source": [
    "#### Matrix Calculation of Self-Attention\n",
    "\n",
    "The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix `X`, and multiplying it by the weight matrices we’ve trained `(WQ, WK, WV)`.\n",
    "\n",
    "<img src=\"self-attention-matrix-calculation.png\" alt=\"matrix-calc\" width=700>\n",
    "\n",
    "Every row in the `X` matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector `(512, or 4 boxes in the figure)`, and the `q/k/v vectors (64, or 3 boxes in the figure)`\n",
    "\n",
    "Finally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.\n",
    "\n",
    "<img src=\"self-attention-matrix-calculation-2.png\" alt=\"matrix-calc2\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8aa9ba-38ab-4c06-9068-0eaf8f121672",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e986446-7f8b-4a46-a521-37d3f872c0f0",
   "metadata": {},
   "source": [
    "#### Decoder-only\n",
    "\n",
    "The decoder-only transformer, which is more commonly-used for modern LLMs, simply removes the encoder from this architecture and uses only the decoder, as indicated by the name. Practically, this means that every layer of the decoder-only transformer architecture contains the following:\n",
    "\n",
    "- A masked self-attention layer.\n",
    "\n",
    "- A feed-forward layer.\n",
    "\n",
    "To form the full decoder-only transformer architecture, we just stack `L` of these layers, which are identical in structure but have independent weights, on top of each other. A depiction of this structure is provided in the figure below.\n",
    "\n",
    "<img src=\"414bf0b5-2043-4fb5-bdab-e0153f893861_1634x808.png\" alt=\"decoder\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a6b79-08f5-472b-8748-b3eba0c3ca3f",
   "metadata": {},
   "source": [
    "Let’s now discuss each component of the architecture in isolation to gain a better understanding. We will start with the input structure for the model, followed by the components of each layer (i.e., self-attention and feed-forward layers) and how they are combined to form the full model architecture.\n",
    "\n",
    "### From Text to Tokens\n",
    "\n",
    "the input to an LLM is just a sequence of text (i.e., the prompt). However, the input that we see in the figure above is not a sequence of text! Rather, the model’s input is a list of token vectors. If we are passing text to the model as input, how do we produce these vectors from our textual input?\n",
    "\n",
    "<img src=\"56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.webp\" alt=\"input_text\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486454a-6f2e-4c4b-9297-b9fb511be2d1",
   "metadata": {},
   "source": [
    "#### Tokenization. \n",
    "\n",
    "The first step of constructing the input for an LLM is breaking the raw textual input—a sequence of characters—into discrete tokens. This process, called tokenization, is handled by the model’s tokenizer. There are many kinds of tokenizers, but Byte-Pair Encoding (BPE) tokenizers [2] are the most common; see here for more details. These tokenizers take a sequence of raw text as input and break this text into a sequence of discrete tokens as shown in the figure above.\n",
    "\n",
    "https://www.youtube.com/watch?v=zduSFxRajkE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac63dc27-a2ed-4e0a-aa12-195f002e58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5473a7fe-f9e2-496c-8187-32ad6db8d25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This raw text will be tokenized\n",
      "Tokens: ['This', 'Ġraw', 'Ġtext', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']\n",
      "Token IDs: [2028, 7257, 1495, 690, 387, 4037, 1534]\n",
      "Token Embeddings Shape: torch.Size([7, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the llama-3.2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "\n",
    "# raw text\n",
    "text = \"This raw text will be tokenized\"\n",
    "\n",
    "# create tokens using tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# token_ids = tokenizer.encode(text)  # directly create token ids\n",
    "\n",
    "# view the results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# create token embedding layer\n",
    "VOCABULARY_SIZE: int = tokenizer.vocab_size\n",
    "EMBEDDING_DIM: int = 768\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=VOCABULARY_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")\n",
    "\n",
    "# get token embeddings (IDs must be passed as a tensor, not a list)\n",
    "token_emb = token_embedding_layer(torch.tensor(token_ids))\n",
    "print(f'Token Embeddings Shape: {token_emb.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007d757-ad47-4550-a1f6-1ad24f9094f1",
   "metadata": {},
   "source": [
    "<img src=\"b8aadf17-3bf6-4b79-9688-b6bfbc5840b1_1830x888.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591387e9-a64b-4abe-a666-42bc4d24fa27",
   "metadata": {},
   "source": [
    "#### Token IDs and Embeddings\n",
    "\n",
    "Each token in the LLM’s vocabulary is associated with a unique integer ID. For example, the prior code yields this sequence of IDs when tokenizing our text: `[2028, 7257, 1495, 690, 387, 4037, 1534]`. Each of these IDs is associated with a vector, known as a token embedding, in an embedding layer. An embedding layer is just a large matrix that stores many rows of vector embeddings. To retrieve the embedding for a token, we just lookup the corresponding row—given by the token ID—in the embedding layer; see above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ff21a-2ee4-4fd8-bf75-df841183ff24",
   "metadata": {},
   "source": [
    "<img src=\"embeddings_positions.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8c1d5c-d616-4219-92ed-cc737db68747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding matrix:\n",
      "tensor([[-0.7294, -0.2191, -0.0981,  ...,  0.3523, -0.1128, -0.8054],\n",
      "        [ 0.6078,  0.7273,  1.0119,  ...,  0.4998,  0.2290,  1.0440],\n",
      "        [-1.6154,  1.4670, -0.2871,  ...,  0.5157, -0.2414, -0.4173],\n",
      "        ...,\n",
      "        [-0.3272,  1.4531,  0.4898,  ..., -0.3586, -0.6169,  0.7317],\n",
      "        [-1.2175,  0.2563, -0.7830,  ..., -0.4600, -0.5081,  0.6184],\n",
      "        [-2.5497,  0.9171,  1.9708,  ..., -0.7857, -0.2745, -1.2188]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Token embedding matrix:')\n",
    "print(token_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c90d7-2d4a-4cbd-b307-ea97c267e8fb",
   "metadata": {},
   "source": [
    "<img src=\"e2f723f2-056a-4fc0-a3f7-7aa151fe297e_1194x1026.png\" alt=\"emb_matrix\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f8cfe-c33c-4a01-a7e8-2e2f8f563063",
   "metadata": {},
   "source": [
    "The token embedding matrix is of size `[C, d]`, where `C` is the number of tokens in our input and `d` is the dimension of token embeddings that is adopted by the LLM. We usually have a batch of `B` input sequences instead of a single input sequence, forming an input matrix of size `[B, C, d]`. The dimension d impacts the sizes of all layers or activations within the transformer, which makes `d` an important hyperparameter choice. Prior to passing this matrix to the transformer as input, we also add a positional embedding to each token in the input, which communicates the position of each token within its sequence to the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78da86-bb77-4a63-b947-102c5bf45c20",
   "metadata": {},
   "source": [
    "#### (Masked and Multi-Headed) Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744d871-c564-4e4d-94bf-b90913134202",
   "metadata": {},
   "source": [
    "<img src=\"e97978e4-cc11-41e0-8fb4-0010039c3769_1456x818.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cddca-a8f6-453e-9a55-cad876e4a7d0",
   "metadata": {},
   "source": [
    "Now, we are ready to pass our input—a token embedding matrix—to the decoder-only transformer to begin processing. As previously outlined, the transformer contains repeated blocks with self-attention and a feed-forward transformation, each followed by normalization operations. Let’s look at self-attention first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fc4c9-66c3-450e-80dc-e4288e7c90a2",
   "metadata": {},
   "source": [
    "<img src=\"d593d54a-21a2-4b60-9b71-73d69f8647a7_1556x820.webp\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc8883-dabf-47ac-b53d-34f04da89279",
   "metadata": {},
   "source": [
    "**What is self-attention?** Put simply, self-attention transforms the representation of each token in a sequence based upon its relationship to other tokens in the sequence. Intuitively, self-attention bases the representation of each token on the other tokens in the sequence (including itself) that are most relevant to that token. In other words, we learn which tokens to “pay attention” to when trying to understand the meaning of a token in our sequence. For example, we see above that the representation for the word making is heavily influenced by the words more and difficult, which help to convey the overall meaning of the sentence.\n",
    "\n",
    "*An attention function maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102480a1-9bad-4dd3-8315-fd85296e73e9",
   "metadata": {},
   "source": [
    "**Scaled Dot Product Attention.** Given our input token matrix of size [C, d] where `C` is # of tokens in input sequence and `d` is embedding dimensions (i.e., we will assume that we are processing a single input sequence instead of a batch for simplicity), we begin by projecting our input using three separate linear projections, forming three separate sets of (transformed) token vectors. These projections are referred to as the key, query and value projections; see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d6267-34b0-4723-a3f8-9b627cc31ea8",
   "metadata": {},
   "source": [
    "<img src=\"d271f9ab-5159-4429-95e4-957db67fe2ec_1360x1286.webp\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce00c8-dfc3-4c40-8a68-18b219559055",
   "metadata": {},
   "source": [
    "The intuitive reasoning for the name of each projection is as follows:\n",
    "\n",
    "A `query` is what you use to search for information. It represents the current token for which we want to find other relevant tokens in the sequence.\n",
    "\n",
    "The `key` represents each other token in the sequence and acts as an index to match the query with other relevant tokens in the sequence.\n",
    "\n",
    "The `value` is the actual information that is retrieved once a query matches a key. The value is used to compute each token’s output in self-attention.\n",
    "\n",
    "The weight matrix `W_q` learns to transform the token embeddings into a space where they encode 'questions' about the input. Extract and emphasize specific dimensions of the token's representation that are most useful for determining relationships with other tokens. Create a representation that determines \"what do I need to focus on (or attend) to understand this input?\n",
    "\n",
    "The weight matrix `W_k` is a transformed representation of each token into a form that indicates: what this token has to offer\" to other tokens in the sequence. \n",
    "\n",
    "**Despite having different names and serving different functions, `Q`, `K`, and `V` initially all come from the same word input embeddings but are transformed differently to serve their specific purposes in the attention mechanism.**\n",
    "\n",
    "Computing attention scores. After projecting the input, we compute an attention score `a[i, j]` for each pair of tokens `[i, j]` in our input sequence. Intuitively, this attention score, which lies in the `[0, 1]` range, captures how much a given token should “pay attention” to another token in the sequence—higher attention scores indicate that a pair of tokens are very relevant to each other. As hinted at above, attention scores are generated using the key and query vectors. We compute `a[i, j]` by taking the dot product of the query vector for token `i` with the key vector for token `j`; see below for a depiction of this process.\n",
    "\n",
    "**Attention scores** = `W_q` . `W_k`. Large values represent close alignment in embedding space. \n",
    "\n",
    "<img src=\"5392813f-9332-4965-83ce-cf75b2ea3cb2_2102x1272.webp\" width=700>\n",
    "\n",
    "<img src=\"attention_scores_matrix.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817b689-8dfc-4887-9292-3b7569bea5b9",
   "metadata": {},
   "source": [
    "We can efficiently compute all pairwise attention scores in a sequence by:\n",
    "\n",
    "- Stacking the query and key vectors into two matrices.\n",
    "\n",
    "- Multiplying the query matrix with the transposed key matrix.\n",
    "\n",
    "This operation forms a matrix of size `[C, C]` called the attention matrix—that contains all pairwise attention scores over the entire sequence. From here, we divide each value in the attention matrix by the square root of `d` an approach that has been found to improve training stability and apply a softmax operation to each row of the attention matrix; see below. After softmax has been applied, each row of the attention matrix forms a valid probability distribution—each row contains positive values that sum to one. The `i-th` row of the attention matrix stores probabilities between the `i-th` token and each other token in our sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245f86b-3328-43d9-b42e-988b02fc5d40",
   "metadata": {},
   "source": [
    "<img src=\"39953be3-b209-44aa-ac88-2a9cc8b6026d_1734x818.webp\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80841c35-c3a9-452e-a6c1-7814c08decee",
   "metadata": {},
   "source": [
    "**Computing output.** \n",
    "\n",
    "Once we have the attention scores, deriving the output of self-attention is easy. The output for each token is simply a weighted combination of value vectors, where the weights are given by the attention scores. To compute this output, we simply multiply the attention matrix by the value matrix as shown above. Notably, self-attention preserves the size of its input—a transformed, `d-dimensional` output vector is produced for each token vector within the input.\n",
    "\n",
    "Masked self-attention. So far, the formulation we have learned is for vanilla (or bidirectional self-attention). As mentioned previously, however, decoder-only transformers use masked self-attention, which modifies the underlying attention pattern by “masking out” tokens that come after each token in the sequence. Each token can only consider tokens that come before it—following tokens are masked.\n",
    "\n",
    "<img src=\"a3d910cc-fd59-45dd-b2b6-9452a6f69bf0_2316x694.webp\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54a697-988f-44ad-a20c-ce7d10977c0a",
   "metadata": {},
   "source": [
    "Let’s consider a token sequence `[“LLM”, “#s”, “are”, “cool”, “.”]` and compute masked attention scores for the token “are”. So far, we have learned that self-attention will compute an attention score between “are” and every other token in the sequence. With masked self-attention, however, we only compute attention scores for “LLM”, “#s”, and “are”. Masked self-attention prohibits us from looking forward in the sequence! Practically, this is achieved by simply setting all attention scores for these tokens to negative infinity, yielding a pairwise probability of zero for masked tokens after the application of softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee0c9f-94e4-4722-bf36-2b336db75cc7",
   "metadata": {},
   "source": [
    "**Attention heads.** The attention operation we have described so far uses softmax to normalize attention scores that are computed across the sequence. Although this approach forms a valid probability distribution, it also limits the ability of self-attention to focus on multiple positions within the sequence—the probability distribution can easily be dominated by one (or a few) words. To solve this issue, we typically compute attention across multiple “heads” in parallel. \n",
    "\n",
    "Within each head, the masked attention operation is identical. However, we:\n",
    "\n",
    "Use separate key, query, and value projections for each attention head.\n",
    "\n",
    "Reduce the dimensionality of the key, query, and value vectors (i.e., this can be done by modifying the linear projection) to reduce computational costs.\n",
    "\n",
    "More specifically, we will change the dimensionality of vectors in each attention head from `d` to `d // H`, where `H` is the number of attention heads, to keep the computational costs of multi-headed self-attention (relatively) fixed.\n",
    "\n",
    "<img src=\"8c1a2682-07ad-4daa-a3ae-f4d3c59d9fb0_2194x992.webp\" width=700>\n",
    "\n",
    "Now, we have several attention heads that compute self-attention in parallel. However, we still need to produce a single output representation from the multiple heads of our self-attention module. We have several options for combining the output of each attention head; e.g., concatenation, averaging, projecting, and more. However, the vanilla implementation of multi-headed self-attention does the following (depicted above):\n",
    "\n",
    "- Concatenates the output of each head.\n",
    "\n",
    "- Linearly projects the concatenated output.\n",
    "\n",
    "Because each attention head outputs token vectors of dimension `d // H`, the concatenated output of all attention heads has dimension `d`. Thus, the multi-headed self-attention operation still preserves the original size of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641d9ba-c9e6-4100-9ce1-1b9072faebc9",
   "metadata": {},
   "source": [
    "https://gist.github.com/wolfecameron/26863dbbc322b15d2e224a2569868256#file-causal_self_attention-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c0c9fb1-b012-4209-b195-9dd9ca5ed0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source: https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d,\n",
    "        H,\n",
    "        T,\n",
    "        bias=False,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        d: size of embedding dimension\n",
    "        H: number of attention heads\n",
    "        T: maximum length of input sequences (in tokens)\n",
    "        bias: whether or not to use bias in linear layers\n",
    "        dropout: probability of dropout\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d % H == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        # output is 3X the dimension because it includes key, query and value\n",
    "        self.c_attn = nn.Linear(d, 3*d, bias=bias)\n",
    "\n",
    "        # projection of concatenated attention head outputs\n",
    "        self.c_proj = nn.Linear(d, d, bias=bias)\n",
    "\n",
    "        # dropout modules\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.H = H\n",
    "        self.d = d\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to\n",
    "        # the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(T, T))\n",
    "                                    .view(1, 1, T, T))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.size() # batch size, sequence length, embedding dimensionality\n",
    "\n",
    "        # compute query, key, and value vectors for all heads in batch\n",
    "        # split the output into separate query, key, and value tensors\n",
    "        q, k, v  = self.c_attn(x).split(self.d, dim=2) # [B, T, d]\n",
    "\n",
    "        # reshape tensor into sequences of smaller token vectors for each head\n",
    "        k = k.view(B, T, self.H, self.d // self.H).transpose(1, 2) # [B, H, T, d // H]\n",
    "        q = q.view(B, T, self.H, self.d // self.H).transpose(1, 2)\n",
    "        v = v.view(B, T, self.H, self.d // self.H).transpose(1, 2)\n",
    "\n",
    "        # compute the attention matrix, perform masking, and apply dropout\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # [B, H, T, T]\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # compute output vectors for each token\n",
    "        y = att @ v # [B, H, T, d // H]\n",
    "\n",
    "        # concatenate outputs from each attention head and linearly project\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.d)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728dc18-78c1-4fd7-b525-78cd21db712d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d31dfa-c8e5-434b-9288-6b198531a216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40cd15-64c2-4076-89f1-9f3fb2990833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896e0db-94f0-44b4-b9e9-3393403ee246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfb152-3733-4286-8e12-ffe8ef309e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101f409-e59b-469e-833e-0f4f3f9046ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b3fa6-f95b-456b-8b26-932725ca4de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73d572-fcb1-4819-98cc-c2fc2b2280e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53193edd-84c4-42ce-9867-18aa40f4076d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0a60f-e435-42a9-ab18-f242f9893d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb869696-083f-427f-bb89-087947bfab9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9672516-d77e-430c-96c3-f9999c24a2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348993a-3f46-4d2a-aa0e-069d3076fd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21885319-ca4c-45fc-8322-602f61eda1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cec527a-c12b-41d0-b914-78111839df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple 2-layer Neural Network\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        # First fully connected layer (Input -> Hidden)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Activation function (ReLU introduces non-linearity)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second fully connected layer (Hidden -> Output)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        :param x: Input tensor\n",
    "        :return: Output tensor (predictions)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)     # First layer transformation\n",
    "        x = self.relu(x)    # Apply ReLU activation\n",
    "        x = self.fc2(x)     # Second layer transformation\n",
    "        return x  # Output layer (logits for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee43d3-3e9b-4989-91fb-6dea01a945a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 3    # Number of input features\n",
    "hidden_size = 5   # Number of neurons in the hidden layer\n",
    "output_size = 2   # Number of output classes\n",
    "\n",
    "# Create the model\n",
    "model = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate some random input (batch size = 4)\n",
    "x_sample = torch.rand(4, input_size)\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model(x_sample)\n",
    "\n",
    "# Print the output\n",
    "print(\"Model Output:\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
