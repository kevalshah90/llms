{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevalshah90/llms/blob/main/10k_chunk_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial setup"
      ],
      "metadata": {
        "id": "ifQs-KCaWh8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96t0sJkExBmT"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i05VEqgkws-s"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain openai unstructured==0.12.5 xmltodict instructor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download TSLA 10-K"
      ],
      "metadata": {
        "id": "p7DQi008WnhC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr1gOy6CxKLZ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "url = \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\"\n",
        "loader = UnstructuredURLLoader(urls=[url], headers={'User-Agent': 'yourname yourname@yourorg.com'})\n",
        "tsla_10k = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define LLM code"
      ],
      "metadata": {
        "id": "miMLxqVQWr-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7xJKerQGCzw"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "You are an expert at extracting text data from SEC filings like 10-Ks, 10-Qs, etc.\n",
        "Your sole job is to take a chunk of text and classify which Item in the SEC filing it belongs to.\n",
        "The goal is to identify and extract the text corresponding to the following Items:\n",
        "- Item 1. Business\n",
        "- Item 1A. Risk Factors\n",
        "- Item 2. Properties\n",
        "- Item 3. Legal Proceedings\n",
        "- Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
        "- Item 7A. Quantitative and Qualitative Disclosures About Market Risk\n",
        "- Item 8. Financial Statements and Supplementary Data\n",
        "\n",
        "Please adhere to the following guidelines:\n",
        "1. Be accurate, factual, and concise with your responses.\n",
        "2. Your response MUST be grounded in truth and the data that is present in the text. Do not make any assumptions or inferences beyond what is explicitly stated in the text.\n",
        "3. If the chunk of text does not clearly belong to any of the specified Items, return an empty list []. Do not attempt to guess or assign an Item if there is insufficient evidence in the text.\n",
        "4. If the chunk of text belongs to multiple Items, return a list of all applicable Items, like [\"Item 1\", \"Item 1A\"].\n",
        "5. Do not provide any explanations or additional information in your response. Only return a list of Items or an empty list.\n",
        "\n",
        "Your response should be formatted as a valid JSON list, like [\"Item 1\", \"Item 1A\"] or [].\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-NcJegeE34i"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import anthropic\n",
        "from openai import OpenAI\n",
        "import instructor\n",
        "\n",
        "openai_client = instructor.patch(OpenAI())\n",
        "\n",
        "\n",
        "class Item(BaseModel):\n",
        "    name: Optional[str]\n",
        "\n",
        "class ItemResponse(BaseModel):\n",
        "    items: List[Item]\n",
        "\n",
        "\n",
        "def classify_items(chunk: str) -> str:\n",
        "  response = openai_client.chat.completions.create(\n",
        "      model=\"gpt-4-0125-preview\",\n",
        "      response_model=ItemResponse,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": prompt,\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"Extract the items from the following chunk: {chunk}\",\n",
        "          },\n",
        "      ],\n",
        "  )\n",
        "  return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk TSLA 10-K"
      ],
      "metadata": {
        "id": "cocClIopWuK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "filing_text = tsla_10k[0].page_content.replace(\"\\n\", \" \")\n",
        "\n",
        "# You can play around with the chunk_size and chunk_overlap to see what works best!\n",
        "chunk_size = 4096\n",
        "chunk_overlap = 400\n",
        "\n",
        "# Split by tokens\n",
        "token_splitter = TokenTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        ")\n",
        "\n",
        "# Chunk the text\n",
        "chunks = token_splitter.split_text(filing_text)"
      ],
      "metadata": {
        "id": "7yXEDxTOKWxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Item extraction"
      ],
      "metadata": {
        "id": "O52uNoWKW_NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\"\"\"\n",
        "Define the output dictionary.\n",
        "Key: Item name.\n",
        "Value: Chunks of text that may belong to the Item.\n",
        "\n",
        "You can modify to include more (or less) Items, if you wish.\n",
        "\"\"\"\n",
        "items_map = {\n",
        "    \"Item 1\": [],\n",
        "    \"Item 1A\": [],\n",
        "    \"Item 2\": [],\n",
        "    \"Item 3\": [],\n",
        "    \"Item 7\": [],\n",
        "    \"Item 7A\": [],\n",
        "    \"Item 8\": [],\n",
        "    \"unknown\": [],\n",
        "}"
      ],
      "metadata": {
        "id": "EbO_ltHwLqVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through all chunks\n",
        "for index, chunk in enumerate(chunks):\n",
        "    # Use LLM to classify the possible Items for the chunk\n",
        "    response = classify_items(chunk)\n",
        "\n",
        "    # Get the possible Items\n",
        "    items = response.items\n",
        "\n",
        "    # Print output for debugging\n",
        "    print(f\"Chunk {index}. Items: {items}\")\n",
        "\n",
        "    # Loop through each item\n",
        "    for item in items:\n",
        "      name = item.name\n",
        "      # Only keep Items that we are expecting\n",
        "      if name in items_map:\n",
        "        items_map[name].append(chunk)\n",
        "\n",
        "    # Prevent rate limiting\n",
        "    time.sleep(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "id": "wZ0Km2rMLjzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the output locally as a .json file"
      ],
      "metadata": {
        "id": "inl3mwtraq8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"tsla_2023_10-k_items.json\""
      ],
      "metadata": {
        "id": "Fp6uSFGeatVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# Assuming you have populated the items_map dictionary with the extracted text\n",
        "\n",
        "# Convert the items_map dictionary to a JSON string\n",
        "json_data = json.dumps(items_map, indent=2)\n",
        "\n",
        "# Write the JSON string to a file\n",
        "with open(\"filename\", \"w\") as file:\n",
        "    file.write(json_data)\n",
        "\n",
        "# Download the JSON file from Google Colab\n",
        "files.download(\"filename\")"
      ],
      "metadata": {
        "id": "wOH7MDYNauu-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}