{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevalshah90/llms/blob/main/gpt_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is heavily inspired from [Sebastian Raschka](https://x.com/rasbt)'s excellent book, \"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\".\n",
        "\n",
        "I highly recommend buying and reading that book.\n",
        "\n",
        "My code here is taken directly from Sebastian's book, but with some slight variable and styling updates.\n",
        "\n",
        "Questions?  Feel free to DM me on [Twitter](https://x.com/virattt)."
      ],
      "metadata": {
        "id": "M5bRPAw1n_oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken --quiet"
      ],
      "metadata": {
        "id": "HuZEvoAjxYNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup the config"
      ],
      "metadata": {
        "id": "Xg68QogFtspy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvXrPrU4q--F"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Implement \"dummy\" transformer"
      ],
      "metadata": {
        "id": "2IOBuc1MxTNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])          # 50257 x 768\n",
        "    self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])   # 1024 x 768\n",
        "    self.drop_embedding = nn.Dropout(config[\"drop_rate\"])\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(config)\n",
        "          for _ in range(config[\"n_layers\"])]\n",
        "    )\n",
        "    self.layer_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, sequence_length = in_idx.shape\n",
        "    token_embeddings = self.token_embedding(in_idx)\n",
        "    position_embeddings = self.position_embedding(\n",
        "      torch.arange(sequence_length, device=in_idx.device)\n",
        "    )\n",
        "    x = token_embeddings + position_embeddings\n",
        "    x = self.drop_embedding(x)\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.layer_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "      super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "IM5IYm1st8VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Test out \"dummy\" transformer"
      ],
      "metadata": {
        "id": "laEWjyCbxdjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "3xsW7vbhv7pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(f\"Output shape: {logits.shape}\")\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "naqM1lRAxhBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Normalize the vector activations"
      ],
      "metadata": {
        "id": "Duj4gbn5yzfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "rK6qBHZLzVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine mean and variance\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(f\"Mean: {mean} \\n\")\n",
        "print(f\"Variance: {var} \\n\")"
      ],
      "metadata": {
        "id": "snA2wzk8Ma-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "out_norm = (out - mean) / torch.sqrt(var) # This is also known as the standard deviation\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "id": "ZdE0tgS2Ms-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "RxG3Sh9NNVYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out the LayerNorm module\n",
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
        "print(f\"Mean: {mean} \\n\")\n",
        "print(f\"Variance: {var} \\n\")"
      ],
      "metadata": {
        "id": "GPM9wZORORXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Implement feed forward network with GELU activations"
      ],
      "metadata": {
        "id": "7UMk2RGQPKeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "loslDi27PNdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "\n",
        "x = torch.linspace(-3, 3, 100)\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    plt.plot(x, y)\n",
        "    plt.title(f\"{label} activation function\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(f\"{label}(x)\")\n",
        "    plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F6fGJIjSSIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement feed-forward neural network\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "5ROLTWSJW55K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it out\n",
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768) # 2 batches, 3 input examples, 768 embeddings per example\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "paZpT47CYM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Implement multi-head attention"
      ],
      "metadata": {
        "id": "TYxPkbofY0gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out                  # 768\n",
        "    self.num_heads = num_heads          # 12\n",
        "    self.head_dim = d_out // num_heads  # 64\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self._out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(\n",
        "            context_length,             # 1024\n",
        "            context_length,             # 1024\n",
        "          ), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, embedding_length = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # Add the num_heads and head_dim dimensions\n",
        "    keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)       # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim) # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)   # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "\n",
        "    # Transpose from (batch_size, num_tokens, num_heads, head_dim) to (batch_size, num_heads, num_tokens, head_dim)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # Calculate attention scores\n",
        "    attention_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Mask the attention scores\n",
        "    attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    # Calculate attention weights\n",
        "    attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    # Apply dropout to attention weights\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Calculate context vectors\n",
        "    context_vectors = (attention_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # Concatenate the context vectors\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, num_tokens, self.d_out)\n",
        "    return self._out_proj(context_vectors)"
      ],
      "metadata": {
        "id": "N3eU4P13Y2ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Implement the Transformer block"
      ],
      "metadata": {
        "id": "jBhdcB3hYXq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(\n",
        "        d_in=config[\"emb_dim\"],\n",
        "        d_out=config[\"emb_dim\"],\n",
        "        context_length=config[\"context_length\"],\n",
        "        dropout=config[\"drop_rate\"],\n",
        "        num_heads=config[\"n_heads\"],\n",
        "        qkv_bias=config[\"qkv_bias\"]\n",
        "    )\n",
        "\n",
        "    self.ff = FeedForward(config)\n",
        "    self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "\n",
        "    # Attention layer\n",
        "    x = self.norm1(x)\n",
        "    x = self.attention(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "\n",
        "    # Feedforward layer\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "    return x"
      ],
      "metadata": {
        "id": "JnzH0rp4Yg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate TransformerBlock and feed it some sample data\n",
        "\n",
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 3, 768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "out = block(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")\n",
        "print(f\"Output: {out}\")"
      ],
      "metadata": {
        "id": "U7UOeMQQcTxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Implement the GPT model"
      ],
      "metadata": {
        "id": "KGmpcEtyFWau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.positional_embedding = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "    self.drop_embedding = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, sequence_length = in_idx.shape\n",
        "    token_embeddings = self.token_embedding(in_idx)\n",
        "    positional_embeddings = self.positional_embedding(\n",
        "        torch.arange(sequence_length, device=in_idx.device)\n",
        "    )\n",
        "    x = token_embeddings + positional_embeddings\n",
        "    x = self.drop_embedding(x)\n",
        "\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "WVfKzAAsgULA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "pOPKlmP3hLC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Generate some text"
      ],
      "metadata": {
        "id": "tFd1M8lCh2cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "RcSZ7SbThZY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ],
      "metadata": {
        "id": "c9zOd0swhu4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=1,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "id": "4WsoMh8Whynf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "_E_9U0FFhzLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKO8DI63h1OM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}