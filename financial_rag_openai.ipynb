{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "UcpUC65WkFY1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevalshah90/llms/blob/main/financial_rag_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "HMx-_kefhEPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Opus grading\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "nwBrCl4-yobn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1DNAGjvg73A"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken unstructured==0.12.5 datasets langchain_anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download SEC filing"
      ],
      "metadata": {
        "id": "3PcCEBN4kI1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "url = \"https://www.sec.gov/Archives/edgar/data/0001559720/000155972023000020/abnb-20230930.htm\"\n",
        "loader = UnstructuredURLLoader(urls=[url], headers={'User-Agent': 'virat virat@virat.com'})\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "VmGucGdHhAii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk and store filing in vector DB"
      ],
      "metadata": {
        "id": "cXwTQca78vtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Naively chunk the SEC filing by tokens\n",
        "token_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=20)\n",
        "docs = token_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "g491xRsu8zfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the chunked docs in vector DB\n",
        "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
      ],
      "metadata": {
        "id": "YNTAHjkA-RcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DataFrame from storage (if exists)"
      ],
      "metadata": {
        "id": "ben3cKdnw1mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_csv_path = \"/content/drive/MyDrive/abnb-2023-annual_report-100-questions.csv\""
      ],
      "metadata": {
        "id": "y9pHHdlNQ3sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = None\n",
        "\n",
        "try:\n",
        "    # Attempt to read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(saved_csv_path)\n",
        "\n",
        "    # Convert contexts from str (default) to list of str\n",
        "    df['contexts'] = df['contexts'].apply(ast.literal_eval)\n",
        "except FileNotFoundError:\n",
        "    # If the file is not found, initialize df to None\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "  print(f\"Loaded DataFrame from storage.\")\n",
        "else:\n",
        "  print(\"No DataFrame found in storage.\")"
      ],
      "metadata": {
        "id": "7L4TqGF_w0o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a test dataset (if stored didn't exist)"
      ],
      "metadata": {
        "id": "UcpUC65WkFY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "\n",
        "if df is None:\n",
        "  # generator with openai models\n",
        "  generator = TestsetGenerator.with_openai()\n",
        "\n",
        "  # IMPORTANT: we use GPT-4 to generate the testset.  So, start with small test_size\n",
        "  test_size = 100\n",
        "\n",
        "  # generate testset\n",
        "  print(\"Generating testset...\")\n",
        "  testset = generator.generate_with_langchain_docs(docs, test_size=test_size, distributions={simple: 0.40, reasoning: 0.40, multi_context: 0.20})\n",
        "  df = testset.to_pandas()\n",
        "\n",
        "  # Mount your Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Save the DataFrame as a CSV file to your Google Drive\n",
        "  df.to_csv(saved_csv_path, index=False)"
      ],
      "metadata": {
        "id": "Se3P_cBsi2w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean DataFrame"
      ],
      "metadata": {
        "id": "GxoLHFF8xPBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the dataset\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "CMjpj5xf-iIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Drop rows with duplicate questions\n",
        "filtered_df = df.drop_duplicates(subset='question', keep='first')\n",
        "\n",
        "# Step 2: Remove rows with missing ground truth\n",
        "condition_nan = pd.isna(filtered_df['ground_truth'])\n",
        "condition_string_nan = filtered_df['ground_truth'].astype(str).str.lower() == 'nan'\n",
        "filtered_df = filtered_df[~(condition_nan | condition_string_nan)]\n",
        "\n",
        "# Step 3: Only keep columns we care about\n",
        "filtered_df = filtered_df[['question', 'contexts', 'ground_truth']]\n",
        "\n",
        "filtered_df.head()"
      ],
      "metadata": {
        "id": "U34F5DQNyGQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate answers using LLM"
      ],
      "metadata": {
        "id": "qYleF6wmuXvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an advanced language model designed to\n",
        "function as a financial assistant with expert-level\n",
        "proficiency in reading and interpreting SEC filings.\n",
        "Your primary role is to assist users in understanding\n",
        "complex financial documents, extracting key information,\n",
        "and providing clear, accurate answers to questions\n",
        "related to these filings.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gvIWBXEXAu0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "def expand_queries(query: str, model=\"gpt-3.5-turbo-0125\") -> List[str]:\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      response_format={\"type\": \"json_object\"},\n",
        "      temperature=0,\n",
        "      seed=42,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a helpful assistant that expands a user query into sub-queries. The sub-queries should be mutually exclusive and collectively exhaustive. Your response will be a JSON object with a `queries` field, which is a list of `query` objects.\"},\n",
        "          {\"role\": \"user\", \"content\": query},\n",
        "      ]\n",
        "  )\n",
        "  return json.loads(response.choices[0].message.content)\n",
        "\n",
        "def rerank_documents(query: str, documents: list, top_k, model=\"gpt-3.5-turbo-0125\") -> List[str]:\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      temperature=0,\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"You are an expert document ranker. Given a query and a list of documents, re-rank the documents by their relevancy to answering the question. Sort the list of documents from most relevant to the question to least relevant.  Only return the top {top_k} documents. Include the full document text in your response\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Query: {query} Documents: {documents}\"}\n",
        "      ]\n",
        "    )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "def answer_question(query: str, documents: list, prompt: str, model=\"gpt-3.5-turbo-0125\") -> str:\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      temperature=0,\n",
        "      seed=42,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": prompt},\n",
        "          {\"role\": \"user\", \"content\": f\"Please answer the question: ```{query}``` given the context: ```{documents}```. Optimize for conciseness and answer correctness.\"},\n",
        "      ]\n",
        "  )\n",
        "  # Get and return the answer\n",
        "  answer = response.choices[0].message.content\n",
        "  return answer"
      ],
      "metadata": {
        "id": "7Uy3nWGmAmhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = filtered_df.iloc[1]['question']\n",
        "ground_truth = filtered_df.iloc[1]['ground_truth']\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Ground Truth: {ground_truth}\")\n",
        "\n",
        "top_k_docs = vectorstore.similarity_search(question, k=5)\n",
        "\n",
        "# Extract the text content from documents\n",
        "documents = [{\"text\": doc.page_content} for doc in top_k_docs]\n",
        "print(f\"Before reranking docs: {documents[0]}\")\n",
        "\n",
        "# Rerank the documents\n",
        "documents = rerank_documents(question, documents, top_k=5)\n",
        "print(f\"After reranking docs: {documents}\")\n",
        "\n",
        "answer = answer_question(question, documents, prompt)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "W12i7p42qfJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "answers = []\n",
        "k = 5\n",
        "\n",
        "# Fields for computing inference speed\n",
        "total_time = 0.0\n",
        "num_iterations = 0\n",
        "\n",
        "# Execute RAG pipeline\n",
        "for index, row in filtered_df.iterrows():\n",
        "  # Get start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Extract the question\n",
        "  question = row['question']\n",
        "\n",
        "  # Print current question\n",
        "  print(f\"Answering question {index + 1}:   {question}\")\n",
        "\n",
        "  # Query vector DB for documents\n",
        "  top_k_docs = vectorstore.similarity_search(question, k)\n",
        "\n",
        "  # Extract the text content from documents\n",
        "  documents = [{\"text\": doc.page_content} for doc in top_k_docs]\n",
        "\n",
        "  # Rerank the documents\n",
        "  documents = rerank_documents(question, documents, k)\n",
        "\n",
        "  # Ask the LLM\n",
        "  answer = answer_question(question, documents, prompt)\n",
        "\n",
        "  # Add generated answer to our list of answers\n",
        "  answers.append(answer)\n",
        "\n",
        "  # Get end time\n",
        "  end_time = time.time()\n",
        "  # Update total execution time (excluding sleep time)\n",
        "  total_time += (end_time - start_time)\n",
        "  num_iterations += 1\n",
        "\n",
        "  # Sleep for 1 second to avoid overloading the LLM\n",
        "  time.sleep(1)\n",
        "\n",
        "# Add the generated answers as a new column in the DataFrame\n",
        "filtered_df['answer'] = answers"
      ],
      "metadata": {
        "id": "BAbQRM8nzb46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average execution time\n",
        "avg_time = total_time / num_iterations\n",
        "\n",
        "print(f\"Took {avg_time} avg seconds for each RAG call\")"
      ],
      "metadata": {
        "id": "FJWt-EWzGzVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visually inspect the answers"
      ],
      "metadata": {
        "id": "4o_GxllnP6dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df"
      ],
      "metadata": {
        "id": "UnZmCoNk_dUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate answers using RAGAS"
      ],
      "metadata": {
        "id": "MChvVtNmusHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert the DataFrame into a HuggingFace DataSet for RAGAS evaluation\n",
        "dataset = Dataset.from_pandas(filtered_df)\n",
        "dataset = dataset.remove_columns('__index_level_0__')"
      ],
      "metadata": {
        "id": "nkcOm0HHu9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use GPT-4 for Evaluation"
      ],
      "metadata": {
        "id": "oHektnWnDcvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_correctness\n",
        "\n",
        "gpt = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
        "\n",
        "gpt_result = evaluate(\n",
        "    dataset,\n",
        "    llm=gpt,\n",
        "    metrics=[answer_correctness],\n",
        ")\n",
        "\n",
        "print(gpt_result)"
      ],
      "metadata": {
        "id": "sN1FcOMDtTvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Claude 3 Opus for Evaluation"
      ],
      "metadata": {
        "id": "t6q6RMbwDZ8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "claude = ChatAnthropic(\n",
        "    temperature=0,\n",
        "    model_name=\"claude-3-opus-20240229\",\n",
        "    anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
        ")\n",
        "\n",
        "claude_result = evaluate(\n",
        "    dataset,\n",
        "    llm=claude,\n",
        "    metrics=[answer_correctness],\n",
        ")\n",
        "\n",
        "print(claude_result)"
      ],
      "metadata": {
        "id": "Vv7SXlAm-H-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get mean of both GPT and Claude scores"
      ],
      "metadata": {
        "id": "RIpVi_cKGFrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average score: {(gpt_result['answer_correctness'] + claude_result['answer_correctness']) / 2}\")"
      ],
      "metadata": {
        "id": "kgjeCLkpFin8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}