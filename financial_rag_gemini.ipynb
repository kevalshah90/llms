{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevalshah90/llms/blob/main/financial_rag_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set your Cohere API gkey\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "HMx-_kefhEPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For GPT-4 grading\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "Joom9Q8dyMgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Gemini\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "UdXvfNU0yFUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Cohere reranker\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "kLpzbdzZ1cMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1DNAGjvg73A"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain cohere ragas arxiv pymupdf chromadb wandb tiktoken unstructured==0.12.5 datasets langchain_anthropic openai pandas google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download SEC filing"
      ],
      "metadata": {
        "id": "3PcCEBN4kI1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "url = \"https://www.sec.gov/Archives/edgar/data/1559720/000155972024000006/abnb-20231231.htm\"\n",
        "loader = UnstructuredURLLoader(urls=[url], headers={'User-Agent': 'virat virat@virat.com'})\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "VmGucGdHhAii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk and store filing in vector DB"
      ],
      "metadata": {
        "id": "cXwTQca78vtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Naively chunk the SEC filing by tokens\n",
        "token_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=20)\n",
        "docs = token_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "g491xRsu8zfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the chunked docs in vector DB\n",
        "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
      ],
      "metadata": {
        "id": "YNTAHjkA-RcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Q&A Dataset"
      ],
      "metadata": {
        "id": "ben3cKdnw1mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the JSON file\n",
        "url = 'https://raw.githubusercontent.com/virattt/datasets/main/abnb-2023-10k.json'\n",
        "\n",
        "# Fetch the JSON content from the URL\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Convert the JSON content to a pandas DataFrame\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "# Rename 'answer' to 'ground_truth' for eval later one\n",
        "df.rename(columns={'answer': 'ground_truth'}, inplace=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "mAZHxmr4-u0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate answers using LLM"
      ],
      "metadata": {
        "id": "qYleF6wmuXvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an expert language model designed to\n",
        "answer questions about financial documents like\n",
        "SEC filings.\n",
        "\n",
        "Given financial documents, your primary role is to extract key information\n",
        "and providing accurate answers to questions\n",
        "related to these filings.\n",
        "\n",
        "In your response, optimize for conciseness, accuracy, and correctness.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gvIWBXEXAu0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "gemini = genai.GenerativeModel('gemini-1.0-pro')\n",
        "response = gemini.generate_content('Please summarise this document: ...')\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "hcL0TIbN0uBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import cohere\n",
        "from typing import List\n",
        "\n",
        "# Setup Gemini\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "gemini = genai.GenerativeModel('gemini-1.0-pro')\n",
        "\n",
        "# Setup Cohere\n",
        "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
        "\n",
        "def rerank_documents(query: str, documents: list, top_k) -> List[str]:\n",
        "  response = co.rerank(\n",
        "      query=query,\n",
        "      documents=documents,\n",
        "      top_n=top_k,\n",
        "      model=\"rerank-english-v2.0\",\n",
        "      return_documents=True\n",
        "  )\n",
        "  results = response.results\n",
        "  return [{\"text\": docs.document.text} for docs in results]\n",
        "\n",
        "from google.generativeai import client\n",
        "\n",
        "def answer_question(query: str, documents: list, prompt: str) -> str:\n",
        "    message = f\"{prompt}. Please answer the question: ```{query}``` using the following documents: ```{documents}```.\"\n",
        "    response = gemini.generate_content(message)\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "7Uy3nWGmAmhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "answers = []\n",
        "k = 3\n",
        "\n",
        "# Fields for computing inference speed\n",
        "total_time = 0.0\n",
        "num_iterations = 0\n",
        "\n",
        "# Execute RAG pipeline\n",
        "for index, row in df.iterrows():\n",
        "  # Get start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Extract the question\n",
        "  question = row['question']\n",
        "\n",
        "  # Print current question\n",
        "  print(f\"Answering question {index + 1}:   {question}\")\n",
        "\n",
        "  # Query vector DB for documents\n",
        "  top_k_docs = vectorstore.similarity_search(question, k)\n",
        "\n",
        "  # Extract the text content from documents\n",
        "  documents = [{\"text\": doc.page_content} for doc in top_k_docs]\n",
        "\n",
        "  # Rerank the documents\n",
        "  documents = rerank_documents(question, documents, k)\n",
        "\n",
        "  # Ask the LLM\n",
        "  answer = answer_question(question, documents, prompt)\n",
        "\n",
        "  # Add generated answer to our list of answers\n",
        "  answers.append(answer)\n",
        "\n",
        "  # Get end time\n",
        "  end_time = time.time()\n",
        "  # Update total execution time (excluding sleep time)\n",
        "  total_time += (end_time - start_time)\n",
        "  num_iterations += 1\n",
        "\n",
        "  # Sleep for 1 second to avoid overloading the LLM\n",
        "  time.sleep(1)\n",
        "\n",
        "# Add the generated answers as a new column in the DataFrame\n",
        "df['answer'] = answers"
      ],
      "metadata": {
        "id": "BAbQRM8nzb46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average execution time\n",
        "avg_time = total_time / num_iterations\n",
        "\n",
        "print(f\"Took {avg_time} avg seconds for each RAG call\")"
      ],
      "metadata": {
        "id": "FrL7WAgZGgPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visually inspect the answers"
      ],
      "metadata": {
        "id": "njvGoWQRXyw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "UnZmCoNk_dUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate answers using RAGAS"
      ],
      "metadata": {
        "id": "MChvVtNmusHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert the DataFrame into a HuggingFace DataSet for RAGAS evaluation\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "nkcOm0HHu9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use GPT-4 for Evaluation"
      ],
      "metadata": {
        "id": "oHektnWnDcvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_correctness\n",
        "\n",
        "gpt = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
        "\n",
        "gpt_result = evaluate(\n",
        "    dataset,\n",
        "    llm=gpt,\n",
        "    metrics=[answer_correctness],\n",
        ")\n",
        "\n",
        "print(gpt_result)"
      ],
      "metadata": {
        "id": "sN1FcOMDtTvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Claude 3 Opus for Evaluation"
      ],
      "metadata": {
        "id": "t6q6RMbwDZ8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "claude = ChatAnthropic(\n",
        "    temperature=0,\n",
        "    model_name=\"claude-3-opus-20240229\",\n",
        "    anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
        ")\n",
        "\n",
        "claude_result = evaluate(\n",
        "    dataset,\n",
        "    llm=claude,\n",
        "    metrics=[answer_correctness],\n",
        ")\n",
        "\n",
        "print(claude_result)"
      ],
      "metadata": {
        "id": "Vv7SXlAm-H-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get mean of both GPT and Claude scores"
      ],
      "metadata": {
        "id": "RIpVi_cKGFrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average score: {(gpt_result['answer_correctness'] + claude_result['answer_correctness']) / 2}\")"
      ],
      "metadata": {
        "id": "kgjeCLkpFin8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YUuybgrfHsmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}