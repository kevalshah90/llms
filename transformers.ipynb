{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb8zwTWfNEW4Vqr4oda+YK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevalshah90/llms/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q09Z4W2GEHxC"
      },
      "outputs": [],
      "source": [
        "import math                    # needed for positional encoding math functions\n",
        "import torch\n",
        "import torch.nn as nn         # high-level neural network modules\n",
        "import torch.optim as optim   # for the Adam optimizer\n",
        "\n",
        "# Run on GPU if available, otherwise CPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ⚙️ Hyperparameters\n",
        "SEQ_LEN = 10     # length of input/output sequences\n",
        "VOCAB_SIZE = 50  # number of unique tokens (IDs 0..VOCAB_SIZE-1)\n",
        "D_MODEL = 32     # embedding/hidden dimension size\n",
        "NHEAD = 4        # number of attention heads\n",
        "NUM_LAYERS = 2   # number of transformer blocks\n",
        "BATCH_SIZE = 16  # number of sequences per training batch\n",
        "EPOCHS = 5       # how many training passes to run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds fixed sinusoidal positional vectors per token position to embeddings,\n",
        "    so the Transformer can understand sequence order (sin(pos/10000^(i/d_model))).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=SEQ_LEN):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model, device=DEVICE)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float, device=DEVICE).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, device=DEVICE).float()\n",
        "                             * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (seq_len, batch_size, d_model)\n",
        "        return x + self.pe[:x.size(0)]"
      ],
      "metadata": {
        "id": "e-K_wXorGl-M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DemoTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, seq_len):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc   = PositionalEncoding(d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=4*d_model,\n",
        "            dropout=0.1,\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.seq_len   = seq_len       # store it internally\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (BATCH_SIZE, T_src), tgt: (BATCH_SIZE, T_tgt)\n",
        "        src = self.token_emb(src).transpose(0,1)  # → (T_src, B, d_model)\n",
        "        src = self.pos_enc(src)\n",
        "        tgt = self.token_emb(tgt).transpose(0,1)  # → (T_tgt, B, d_model)\n",
        "        tgt = self.pos_enc(tgt)\n",
        "\n",
        "        T_tgt = tgt.size(0)  # actual target length\n",
        "        # mask prevents each position seeing future positions\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(T_tgt).to(tgt.device)\n",
        "\n",
        "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
        "        out = self.fc_out(out)        # (T_tgt, B, vocab_size)\n",
        "        return out.transpose(0,1)     # → (B, T_tgt, vocab_size)"
      ],
      "metadata": {
        "id": "ttw0tkQiGqCH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_demo():\n",
        "    model = DemoTransformer(VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS, SEQ_LEN).to(DEVICE)\n",
        "    # CrossEntropyLoss combines log-softmax + negative log likelihood in one efficient op   [oai_citation:7‡Reddit](https://www.reddit.com/r/MLQuestions/comments/ledtzw/torchnncrossentropyloss/?utm_source=chatgpt.com)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(\"Training on synthetic data…\")\n",
        "    model.train()\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # Generate random token sequences in [0, VOCAB_SIZE)\n",
        "        src = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN), device=DEVICE)\n",
        "        tgt_input = src[:, :-1]  # decoder input (everything except last token)\n",
        "        tgt_output = src[:, 1:]  # target next-token labels\n",
        "\n",
        "        logits = model(src, tgt_input)   # (BATCH, seq_len-1, vocab_size)\n",
        "\n",
        "        # Prepare for CrossEntropyLoss:\n",
        "        # - logits must be shape (N, C) where N = BATCH*(seq_len-1), C = vocab_size\n",
        "        # - targets must be shape (N,) with integer class indices\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, VOCAB_SIZE),\n",
        "            tgt_output.reshape(-1)\n",
        "        )\n",
        "        # Internally, CrossEntropyLoss does:\n",
        "        #   1. LogSoftmax on logits → log-probabilities\n",
        "        #   2. Picks log(p_true) for each position\n",
        "        #   3. Takes negative and averages over samples 🡪 hence lower loss = better performance  [oai_citation:8‡Zero To Mastery](https://zerotomastery.io/blog/pytorch-loss-functions/?utm_source=chatgpt.com) [oai_citation:9‡PyTorch Forums](https://discuss.pytorch.org/t/how-does-nn-crossentropyloss-work-under-the-hood/126431?utm_source=chatgpt.com)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"  epoch {epoch}/{EPOCHS}, loss = {loss.item():.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def greedy_decode(model):\n",
        "    \"\"\"\n",
        "    Demonstrates step-by-step generation by greedy decoding:\n",
        "    at each timestep, we feed the previously generated tokens into the decoder.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Seed sequence: random\n",
        "        input_seq = torch.randint(0, VOCAB_SIZE, (1, SEQ_LEN), device=DEVICE)\n",
        "        output_seq = [input_seq[0, 0].item()]  # start with the first token from seed\n",
        "\n",
        "        for t in range(1, SEQ_LEN):\n",
        "            src = input_seq[:, :t]  # feed all previous tokens as src\n",
        "            tgt = torch.tensor([output_seq + [0] * (SEQ_LEN - len(output_seq))],\n",
        "                               dtype=torch.long, device=DEVICE)\n",
        "            logits = model(src, tgt)  # shape: (1, seq_len, vocab_size)\n",
        "            next_tok = logits[0, t - 1].argmax().item()\n",
        "            output_seq.append(next_tok)\n",
        "\n",
        "        print(\"Input (first seed):\", input_seq[0].tolist())\n",
        "        print(\"Greedy output:\", output_seq)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    m = train_demo()\n",
        "    greedy_decode(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otEz_aDGGxNs",
        "outputId": "77a0a759-24bd-40c2-802e-b9ba7433fe41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on synthetic data…\n",
            "  epoch 1/5, loss = 4.2954\n",
            "  epoch 2/5, loss = 4.2355\n",
            "  epoch 3/5, loss = 4.2458\n",
            "  epoch 4/5, loss = 4.1537\n",
            "  epoch 5/5, loss = 4.0078\n",
            "Input (first seed): [42, 36, 29, 26, 46, 29, 24, 18, 34, 30]\n",
            "Greedy output: [42, 23, 23, 23, 23, 25, 25, 25, 25, 39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}